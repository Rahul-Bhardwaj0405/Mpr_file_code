***********************SIMPLE ****************************************************************
import json
import pandas as pd
from io import BytesIO
from celery import shared_task
from django.db import transaction, connection
from django.core.cache import cache
import logging

logger = logging.getLogger(__name__)

# Cache timeout in seconds
CACHE_TIMEOUT = 3600

def fetch_mapping_and_rules(bank_name, transaction_type):
    cache_key = f"mapping_rules_{bank_name}_{transaction_type}"
    cached_data = cache.get(cache_key)
    if cached_data:
        logger.info(f"Using cached mapping and rules for {bank_name}, {transaction_type}")
        return cached_data

    query = """
        SELECT header_mapping, validation_rules
        FROM bank_transaction_mappings
        WHERE bank_name = %s AND transaction_type = %s
    """
    with connection.cursor() as cursor:
        cursor.execute(query, [bank_name, transaction_type])
        result = cursor.fetchone()

    if result:
        header_mapping = json.loads(result[0])
        validation_rules = json.loads(result[1])
        cache.set(cache_key, (header_mapping, validation_rules), timeout=CACHE_TIMEOUT)
        return header_mapping, validation_rules

    logger.error(f"No mapping found for bank: {bank_name}, transaction type: {transaction_type}")
    return None, None

def clean_column_name(column_name):
    column_cleaning_regex = re.compile(r'\[.*?\]')
    cleaned_name = column_cleaning_regex.sub('', column_name)
    cleaned_name = ''.join(part for part in cleaned_name.split() if part)
    cleaned_name = ''.join(char for char in cleaned_name if char not in ['.', '_']).strip()
    return cleaned_name

def convert_column_to_datetime(df, column_name):
    df[column_name] = pd.to_datetime(df[column_name], errors='coerce')
    unsuccessful_dates = df[df[column_name].isna()]
    if not unsuccessful_dates.empty:
        logger.warning(f"Could not parse dates in column '{column_name}'; rows affected: {unsuccessful_dates.index.tolist()}")
    return df

def convert_payable_to_float(df, column_name):
    df[column_name] = pd.to_numeric(df[column_name].str.replace(',', ''), errors='coerce')
    return df

def process_dataframe_chunk(df_chunk, header_mapping, validation_rules, bank_name, transaction_type, merchant_name):
    logger.info(f"Processing DataFrame chunk for bank: {bank_name}, transaction_type: {transaction_type}")

    # Clean column names
    df_chunk.columns = [clean_column_name(col) for col in df_chunk.columns]

    # Validate required columns and apply mappings
    for column, rules in validation_rules.items():
        if "required" in rules and rules["required"] and column not in df_chunk.columns:
            logger.error(f"Missing required column '{column}' in file for bank: {bank_name}, transaction_type: {transaction_type}")
            return
        if "index" in rules:
            expected_index = rules["index"]
            actual_index = list(df_chunk.columns).index(column) if column in df_chunk.columns else None
            if actual_index != expected_index:
                logger.error(f"Column '{column}' is not at expected index {expected_index}. Found: {actual_index}")
                return

    # Rename columns based on mapping
    df_chunk.rename(columns=header_mapping, inplace=True)

    # Apply data type conversions
    if 'Transaction_Date' in df_chunk.columns:
        df_chunk = convert_column_to_datetime(df_chunk, 'Transaction_Date')
    if 'Settlement_Date' in df_chunk.columns:
        df_chunk = convert_column_to_datetime(df_chunk, 'Settlement_Date')
    if 'Payable_Merchant' in df_chunk.columns:
        df_chunk = convert_payable_to_float(df_chunk, 'Payable_Merchant')

    # Apply bank-specific logic
    if bank_name == 'icici':
        df_chunk['Payable_Merchant'] = df_chunk['Payable_Merchant'].fillna(0)
        df_chunk['CREDIT_DEBIT_AMOUNT'] = df_chunk['Payable_Merchant'].apply(
            lambda x: 'CREDIT' if x > 0 else 'DEBIT' if x < 0 else None
        )
        logger.debug(f"ICICI-specific CREDIT/DEBIT logic applied: {df_chunk[['Payable_Merchant', 'CREDIT_DEBIT_AMOUNT']].head()}")

    # Process transactions in the chunk
    process_transactions(df_chunk, bank_name, transaction_type, merchant_name)

def process_transactions(df_chunk, bank_name, transaction_type, merchant_name):
    logger.info(f"Started processing transactions for bank: {bank_name}, transaction_type: {transaction_type}, merchant_name: {merchant_name}")
    bulk_data_transactions = []
    bank_id = BANK_CODE_MAPPING.get(bank_name)

    if not bank_id:
        logger.error(f"No bank ID found for bank: {bank_name}")
        return

    for _, row in df_chunk.iterrows():
        transaction_data = {
            'Transaction_type': transaction_type,
            'Merchant_Name': merchant_name,
            'MID': row.get('MID', bank_id),
            'Transaction_Id': row.get('Transaction_Id'),
            'Order_Id': row.get('Order_Id'),
            'Transaction_Date': row.get('Transaction_Date'),
            'Settlement_Date': row.get('Settlement_Date'),
            'Payable_Merchant': row.get('Payable_Merchant'),
            'CREDIT_DEBIT_AMOUNT': row.get('CREDIT_DEBIT_AMOUNT'),
            'Bank_Name': bank_name,
        }
        bulk_data_transactions.append(transaction_data)

    if bulk_data_transactions:
        try:
            with transaction.atomic():
                Transaction.bulk_create_transactions(bulk_data_transactions)
            logger.info(f"Processed {len(bulk_data_transactions)} transactions successfully.")
        except Exception as e:
            logger.error(f"Failed to save transactions to the database: {e}", exc_info=True)

@shared_task
def process_uploaded_files(file_contents, bank_name, transaction_type, file_formats, merchant_name):
    logger.info(f"Starting file processing for bank: {bank_name}, transaction_type: {transaction_type}")
    try:
        header_mapping, validation_rules = fetch_mapping_and_rules(bank_name, transaction_type)
        if not header_mapping or not validation_rules:
            logger.error(f"Cannot proceed without mappings or validation rules for {bank_name}, {transaction_type}")
            return

        for file_index, (file_content, file_name) in enumerate(file_contents):
            try:
                if file_formats[file_index] == 'excel':
                    df = pd.read_excel(BytesIO(file_content), dtype=str, engine='openpyxl')
                    df_chunks = [df]
                elif file_formats[file_index] == 'csv':
                    df_chunks = pd.read_csv(BytesIO(file_content), dtype=str, chunksize=100000)

                for df_chunk in df_chunks:
                    process_dataframe_chunk(df_chunk, header_mapping, validation_rules, bank_name, transaction_type, merchant_name)
            except Exception as e:
                logger.error(f"Error processing file: {file_name}. Exception: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"Error during file processing. Exception: {e}", exc_info=True)
        raise
*********************************************************************************************************************************************************


import json
import pandas as pd
from io import BytesIO
from celery import shared_task
from django.db import transaction, connection
from django.core.cache import cache
import logging

logger = logging.getLogger(__name__)

# Cache timeout in seconds
CACHE_TIMEOUT = 3600

def fetch_mapping_and_rules(bank_name, transaction_type):
    cache_key = f"mapping_rules_{bank_name}_{transaction_type}"
    cached_data = cache.get(cache_key)
    if cached_data:
        logger.info(f"Using cached mapping and rules for {bank_name}, {transaction_type}")
        return cached_data

    query = """
        SELECT header_mapping, validation_rules
        FROM bank_transaction_mappings
        WHERE bank_name = %s AND transaction_type = %s
    """
    with connection.cursor() as cursor:
        cursor.execute(query, [bank_name, transaction_type])
        result = cursor.fetchone()

    if result:
        header_mapping = json.loads(result[0])
        validation_rules = json.loads(result[1])
        cache.set(cache_key, (header_mapping, validation_rules), timeout=CACHE_TIMEOUT)
        return header_mapping, validation_rules

    logger.error(f"No mapping found for bank: {bank_name}, transaction type: {transaction_type}")
    return None, None

def clean_column_name(column_name):
    # Optimized column name cleaning without multiple iterations
    return ''.join(char for char in column_name if char.isalnum()).strip()

def process_dataframe_chunk(df_chunk, header_mapping, validation_rules, bank_name, transaction_type, merchant_name):
    logger.info(f"Processing DataFrame chunk for bank: {bank_name}, transaction_type: {transaction_type}")

    # Clean column names in one operation
    df_chunk.columns = [clean_column_name(col) for col in df_chunk.columns]

    # Validate required columns and apply mappings
    missing_columns = [col for col, rules in validation_rules.items() if "required" in rules and rules["required"] and col not in df_chunk.columns]
    if missing_columns:
        logger.error(f"Missing required columns {missing_columns} for bank: {bank_name}, transaction_type: {transaction_type}")
        return

    # Rename columns based on mapping
    df_chunk.rename(columns=header_mapping, inplace=True)

    # Optimize data type conversions using vectorized operations
    if 'Transaction_Date' in df_chunk.columns:
        df_chunk['Transaction_Date'] = pd.to_datetime(df_chunk['Transaction_Date'], errors='coerce')
    if 'Settlement_Date' in df_chunk.columns:
        df_chunk['Settlement_Date'] = pd.to_datetime(df_chunk['Settlement_Date'], errors='coerce')
    if 'Payable_Merchant' in df_chunk.columns:
        df_chunk['Payable_Merchant'] = pd.to_numeric(df_chunk['Payable_Merchant'].str.replace(',', ''), errors='coerce')

    # Apply bank-specific logic in vectorized form
    if bank_name == 'icici' and 'Payable_Merchant' in df_chunk.columns:
        df_chunk['Payable_Merchant'] = df_chunk['Payable_Merchant'].fillna(0)
        df_chunk['CREDIT_DEBIT_AMOUNT'] = df_chunk['Payable_Merchant'].apply(
            lambda x: 'CREDIT' if x > 0 else 'DEBIT' if x < 0 else None
        )

    # Bulk insert transactions
    process_transactions(df_chunk, bank_name, transaction_type, merchant_name)

def process_transactions(df_chunk, bank_name, transaction_type, merchant_name):
    logger.info(f"Started processing transactions for bank: {bank_name}, transaction_type: {transaction_type}, merchant_name: {merchant_name}")
    bank_id = BANK_CODE_MAPPING.get(bank_name)

    if not bank_id:
        logger.error(f"No bank ID found for bank: {bank_name}")
        return

    # Create a bulk data dictionary using pandas for faster access
    df_chunk['Bank_Name'] = bank_name
    df_chunk['Transaction_type'] = transaction_type
    df_chunk['Merchant_Name'] = merchant_name
    df_chunk['MID'] = bank_id
    bulk_data_transactions = df_chunk.to_dict(orient='records')

    if bulk_data_transactions:
        try:
            with transaction.atomic():
                Transaction.bulk_create_transactions(bulk_data_transactions)
            logger.info(f"Processed {len(bulk_data_transactions)} transactions successfully.")
        except Exception as e:
            logger.error(f"Failed to save transactions to the database: {e}", exc_info=True)

@shared_task
def process_uploaded_files(file_contents, bank_name, transaction_type, file_formats, merchant_name):
    logger.info(f"Starting file processing for bank: {bank_name}, transaction_type: {transaction_type}")
    try:
        header_mapping, validation_rules = fetch_mapping_and_rules(bank_name, transaction_type)
        if not header_mapping or not validation_rules:
            logger.error(f"Cannot proceed without mappings or validation rules for {bank_name}, {transaction_type}")
            return

        for file_index, (file_content, file_name) in enumerate(file_contents):
            try:
                if file_formats[file_index] == 'excel':
                    df_chunks = pd.read_excel(BytesIO(file_content), dtype=str, engine='openpyxl', chunksize=100000)
                elif file_formats[file_index] == 'csv':
                    df_chunks = pd.read_csv(BytesIO(file_content), dtype=str, chunksize=100000)

                for df_chunk in df_chunks:
                    process_dataframe_chunk(df_chunk, header_mapping, validation_rules, bank_name, transaction_type, merchant_name)
            except Exception as e:
                logger.error(f"Error processing file: {file_name}. Exception: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"Error during file processing. Exception: {e}", exc_info=True)
        raise

